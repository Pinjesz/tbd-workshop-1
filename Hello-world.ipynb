{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dd905e7-94e2-4666-bbf2-33cc9178f5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.databricks#spark-xml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-08e1d180-adf8-491b-8e6e-e2f99406398d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#spark-xml_2.12;0.17.0 in central\n",
      "\tfound commons-io#commons-io;2.11.0 in central\n",
      "\tfound org.glassfish.jaxb#txw2;3.0.2 in central\n",
      "\tfound org.apache.ws.xmlschema#xmlschema-core;2.3.0 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.9.0 in central\n",
      "downloading https://repo1.maven.org/maven2/com/databricks/spark-xml_2.12/0.17.0/spark-xml_2.12-0.17.0.jar ...\n",
      "\t[SUCCESSFUL ] com.databricks#spark-xml_2.12;0.17.0!spark-xml_2.12.jar (26ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-io/commons-io/2.11.0/commons-io-2.11.0.jar ...\n",
      "\t[SUCCESSFUL ] commons-io#commons-io;2.11.0!commons-io.jar (23ms)\n",
      "downloading https://repo1.maven.org/maven2/org/glassfish/jaxb/txw2/3.0.2/txw2-3.0.2.jar ...\n",
      "\t[SUCCESSFUL ] org.glassfish.jaxb#txw2;3.0.2!txw2.jar (21ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/ws/xmlschema/xmlschema-core/2.3.0/xmlschema-core-2.3.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.ws.xmlschema#xmlschema-core;2.3.0!xmlschema-core.jar(bundle) (28ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scala-lang/modules/scala-collection-compat_2.12/2.9.0/scala-collection-compat_2.12-2.9.0.jar ...\n",
      "\t[SUCCESSFUL ] org.scala-lang.modules#scala-collection-compat_2.12;2.9.0!scala-collection-compat_2.12.jar (27ms)\n",
      ":: resolution report :: resolve 5927ms :: artifacts dl 153ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-xml_2.12;0.17.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.11.0 from central in [default]\n",
      "\torg.apache.ws.xmlschema#xmlschema-core;2.3.0 from central in [default]\n",
      "\torg.glassfish.jaxb#txw2;3.0.2 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.9.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   5   |   5   |   0   ||   5   |   5   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-08e1d180-adf8-491b-8e6e-e2f99406398d\n",
      "\tconfs: [default]\n",
      "\t5 artifacts copied, 0 already retrieved (989kB/27ms)\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.hadoop.shaded.org.xbill.DNS.ResolverConfig (file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-client-runtime-3.3.2.jar) to method sun.net.dns.ResolverConfiguration.open()\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.shaded.org.xbill.DNS.ResolverConfig\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/15 16:39:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/15 16:39:41 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "24/04/15 16:39:41 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "24/04/15 16:39:51 WARN Client: Same path resource file:///root/.ivy2/jars/com.databricks_spark-xml_2.12-0.17.0.jar added multiple times to distributed cache.\n",
      "24/04/15 16:39:51 WARN Client: Same path resource file:///root/.ivy2/jars/commons-io_commons-io-2.11.0.jar added multiple times to distributed cache.\n",
      "24/04/15 16:39:51 WARN Client: Same path resource file:///root/.ivy2/jars/org.glassfish.jaxb_txw2-3.0.2.jar added multiple times to distributed cache.\n",
      "24/04/15 16:39:51 WARN Client: Same path resource file:///root/.ivy2/jars/org.apache.ws.xmlschema_xmlschema-core-2.3.0.jar added multiple times to distributed cache.\n",
      "24/04/15 16:39:51 WARN Client: Same path resource file:///root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.9.0.jar added multiple times to distributed cache.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TBD-workshop-1-gr3\").master(\"yarn\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c03e7d9-369c-4066-8c64-d666283f5fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/15 16:40:11 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.10.10.4:39402) with ID 2,  ResourceProfileId 0\n",
      "24/04/15 16:40:12 INFO BlockManagerMasterEndpoint: Registering block manager tbd-cluster-w-1.c.tbd-2024l-303760.internal:39361 with 1048.8 MiB RAM, BlockManagerId(2, tbd-cluster-w-1.c.tbd-2024l-303760.internal, 39361, None)\n"
     ]
    }
   ],
   "source": [
    "spark.sparkContext.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c71e09ed-388e-48ec-9ef0-615d194fc719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/15 16:40:26 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.10.10.2:54832) with ID 1,  ResourceProfileId 0\n",
      "24/04/15 16:40:26 INFO SparkContext: Starting job: count at /tmp/ipykernel_110/2992961663.py:1\n",
      "24/04/15 16:40:26 INFO BlockManagerMasterEndpoint: Registering block manager tbd-cluster-w-0.c.tbd-2024l-303760.internal:35097 with 1048.8 MiB RAM, BlockManagerId(1, tbd-cluster-w-0.c.tbd-2024l-303760.internal, 35097, None)\n",
      "24/04/15 16:40:26 INFO DAGScheduler: Got job 0 (count at /tmp/ipykernel_110/2992961663.py:1) with 2 output partitions\n",
      "24/04/15 16:40:26 INFO DAGScheduler: Final stage: ResultStage 0 (count at /tmp/ipykernel_110/2992961663.py:1)\n",
      "24/04/15 16:40:26 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/04/15 16:40:26 INFO DAGScheduler: Missing parents: List()\n",
      "24/04/15 16:40:26 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[1] at count at /tmp/ipykernel_110/2992961663.py:1), which has no missing parents\n",
      "24/04/15 16:40:26 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 7.4 KiB, free 434.4 MiB)\n",
      "24/04/15 16:40:26 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.4 MiB)\n",
      "24/04/15 16:40:26 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.10.10.2:54844) with ID 3,  ResourceProfileId 0\n",
      "24/04/15 16:40:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on tbd-2024l-303760-notebook.c.tbd-2024l-303760.internal:16385 (size: 4.6 KiB, free: 434.4 MiB)\n",
      "24/04/15 16:40:26 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1513\n",
      "24/04/15 16:40:26 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[1] at count at /tmp/ipykernel_110/2992961663.py:1) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/04/15 16:40:26 INFO YarnScheduler: Adding task set 0.0 with 2 tasks resource profile 0\n",
      "24/04/15 16:40:26 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (tbd-cluster-w-1.c.tbd-2024l-303760.internal, executor 2, partition 0, PROCESS_LOCAL, 4471 bytes) taskResourceAssignments Map()\n",
      "24/04/15 16:40:26 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (tbd-cluster-w-0.c.tbd-2024l-303760.internal, executor 1, partition 1, PROCESS_LOCAL, 4498 bytes) taskResourceAssignments Map()\n",
      "24/04/15 16:40:27 INFO BlockManagerMasterEndpoint: Registering block manager tbd-cluster-w-0.c.tbd-2024l-303760.internal:37903 with 1048.8 MiB RAM, BlockManagerId(3, tbd-cluster-w-0.c.tbd-2024l-303760.internal, 37903, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/15 16:40:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on tbd-cluster-w-1.c.tbd-2024l-303760.internal:39361 (size: 4.6 KiB, free: 1048.8 MiB)\n",
      "24/04/15 16:40:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on tbd-cluster-w-0.c.tbd-2024l-303760.internal:35097 (size: 4.6 KiB, free: 1048.8 MiB)\n",
      "24/04/15 16:40:29 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2319 ms on tbd-cluster-w-1.c.tbd-2024l-303760.internal (executor 2) (1/2)\n",
      "24/04/15 16:40:29 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 39557\n",
      "24/04/15 16:40:29 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2536 ms on tbd-cluster-w-0.c.tbd-2024l-303760.internal (executor 1) (2/2)\n",
      "24/04/15 16:40:29 INFO DAGScheduler: ResultStage 0 (count at /tmp/ipykernel_110/2992961663.py:1) finished in 2.866 s\n",
      "24/04/15 16:40:29 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/04/15 16:40:29 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "24/04/15 16:40:29 INFO YarnScheduler: Killing all running tasks in stage 0: Stage finished\n",
      "24/04/15 16:40:29 INFO DAGScheduler: Job 0 finished: count at /tmp/ipykernel_110/2992961663.py:1, took 2.992955 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize((1,1,1)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de2926ec-9c36-4d89-b152-927fe5ba2d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/15 16:40:40 INFO SparkUI: Stopped Spark web UI at http://tbd-2024l-303760-notebook.c.tbd-2024l-303760.internal:4040\n",
      "24/04/15 16:40:40 INFO YarnClientSchedulerBackend: Interrupting monitor thread\n",
      "24/04/15 16:40:40 INFO YarnClientSchedulerBackend: Shutting down all executors\n",
      "24/04/15 16:40:40 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\n",
      "24/04/15 16:40:40 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped\n",
      "24/04/15 16:40:40 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "24/04/15 16:40:40 INFO MemoryStore: MemoryStore cleared\n",
      "24/04/15 16:40:40 INFO BlockManager: BlockManager stopped\n",
      "24/04/15 16:40:40 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "24/04/15 16:40:40 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "24/04/15 16:40:40 INFO SparkContext: Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e23c79-2943-4cd3-8168-c599266655af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
